{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import overload, Optional, Union, List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### logging "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `A.B` logger inheritance to control logging level from one entry point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_FMT = '[{name}][{asctime} {msecs:0>7.3f}][{levelname}]-pid:{process} th_id:{thread}- {message!s}'\n",
    "DATE_FMT = '%y/%m/%d %H:%M:%S'\n",
    "LOGGER_NAME = 'cnR_async'\n",
    "OUTPUT_PATH = 'cnReuters_output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_logger(base_name: str = LOGGER_NAME, \n",
    "                    level: int = logging.DEBUG) -> logging.Logger:\n",
    "    logger = logging.getLogger(base_name)\n",
    "    logger.setLevel(level)\n",
    "    # only add handlers if there is none, prevent duplicate outputs\n",
    "    if not logger.hasHandlers():\n",
    "        handler = logging.StreamHandler(stream=sys.stdout)\n",
    "        handler.setFormatter(\n",
    "            logging.Formatter(fmt=MAIN_FMT, datefmt=DATE_FMT, style='{')\n",
    "        )\n",
    "        handler.setLevel(level)\n",
    "        logger.addHandler(handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def get_logger(name: str, base_name: str = LOGGER_NAME) -> logging.Logger:\n",
    "    logger = logging.getLogger(f'{base_name}.{name}')\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_logger = get_main_logger('test', logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test][20/04/23 14:51:47 899.773][DEBUG]-pid:18068 th_id:15612- test\n"
     ]
    }
   ],
   "source": [
    "test_logger.debug('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _replace_to_empty(s: str, *to_replace: str) -> str:\n",
    "    \"\"\"del multiple chars in a string\"\"\"\n",
    "    for chars in to_replace:\n",
    "        s = s.replace(chars, '')\n",
    "    return s\n",
    "\n",
    "\n",
    "def _replace(s: str) -> str:\n",
    "    return _replace_to_empty(s, '\\n', '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path: str, logger: logging.Logger) -> None:\n",
    "    if path in os.listdir():\n",
    "        logger.info(f'path {path} already exists')\n",
    "    else:\n",
    "        os.mkdir(os.path.join('.', path))\n",
    "        logger.info(f\"{os.path.join(os.path.abspath('.'), path)} created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetLogger:\n",
    "    \n",
    "    logger: logging.Logger\n",
    "        \n",
    "    @classmethod\n",
    "    def set_logger(cls, name: str) -> None:\n",
    "        cls.logger = get_logger(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### async_request with aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# headers constants\n",
    "HEADERS = {\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'cross-site',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36'\n",
    "}\n",
    "BASE_URL = 'https://cn.reuters.com/news/archive/topic-cn-top-news?view=page&page={}&pageSize=10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an async request\n",
    "* provide async session as param\n",
    "* catch exceptions related to connection and request\n",
    "* return text   \n",
    "\n",
    "Delay should be realised in the worker function with async queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@overload\n",
    "async def async_get(url: str, \n",
    "                    msg: str, \n",
    "                    headers: dict, \n",
    "                    session: aiohttp.ClientSession, \n",
    "                    logger: logging.Logger) -> Optional[bytes]: ...\n",
    "\n",
    "\n",
    "@overload\n",
    "async def async_get(url: str, \n",
    "                    msg: str, \n",
    "                    headers: dict, \n",
    "                    session: aiohttp.ClientSession, \n",
    "                    logger: logging.Logger,\n",
    "                    *,\n",
    "                    text: bool) -> Optional[str]: ...\n",
    "\n",
    "\n",
    "async def async_get(url: str, \n",
    "                    msg: str, \n",
    "                    headers: dict, \n",
    "                    session: aiohttp.ClientSession, \n",
    "                    logger: logging.Logger,\n",
    "                    *,\n",
    "                    text: Optional[bool] = None) -> Optional[Union[str, bytes]]:\n",
    "    t1 = time.time()\n",
    "    returned: Union[bytes, str]\n",
    "    try:\n",
    "        async with session.get(url, headers=headers) as resp:\n",
    "            if not text:\n",
    "                # binary mode\n",
    "                returned = await resp.read()\n",
    "            else:\n",
    "                returned = await resp.text(encoding='utf-8')\n",
    "            if resp.status >= 400:\n",
    "                logger.warning(f'{msg} [{resp.status} {resp.reason} {time.time()-t1:.5f}s] ')\n",
    "            else:\n",
    "                logger.info(f'{msg} [{resp.status} {resp.reason} {time.time()-t1:.5f}s] ')\n",
    "            return returned\n",
    "    except aiohttp.ClientError as ce:\n",
    "        logger.exception(ce)\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test][20/04/23 14:51:49 469.548][WARNING]-pid:18068 th_id:15612- test p1 [404 NOT FOUND 0.96737s] \n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "_test_url = 'https://httpbin.org/status/404'\n",
    "async with aiohttp.ClientSession() as s:\n",
    "    _test_return = await async_get(_test_url, 'test p1', HEADERS, s, test_logger, text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page/Entry object and Url construct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Entry` represents each title in a page, includes the following:\n",
    "* should have these attributes:\n",
    " * date, time (not necessary, only for latest news), title, content, tag    \n",
    " \n",
    "Rules for keys:   \n",
    "* if *time* exits, use today's date\n",
    "* tag:\n",
    " * configurable classification processor\n",
    " * one and only one tag for each `Entry`\n",
    " * if cannot be classified, fill tag with `'untagged'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure of a json configuration file:   \n",
    "* tag_name: \\[specific words to mark a tag\\]\n",
    "```\n",
    "{\n",
    "    tag_conf: [\n",
    "        {tag_name_1: [word1, word2, ...]},\n",
    "        ...\n",
    "        {tag_name_n: [word1, word2, ...]}\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entry(SetLogger):\n",
    "    \n",
    "    page: int\n",
    "    index: int\n",
    "    time: str\n",
    "    date: str\n",
    "    title: str\n",
    "    content: str\n",
    "    tag: str\n",
    "    configure: dict = dict()\n",
    "    \n",
    "    def __init__(self, page: int, index: int, \n",
    "                 time_: str, date: str, title: str, content: str):\n",
    "        self.page = page\n",
    "        self.index = index\n",
    "        self.time = time_\n",
    "        self.date = date\n",
    "        self.title = title\n",
    "        self.content = content\n",
    "        # default empty tag\n",
    "        self.tag = ''\n",
    "    \n",
    "    def get_tag(self) -> None:\n",
    "        tag_dict: Dict[str, List[str]]\n",
    "        try:\n",
    "            if self.configure:\n",
    "                for tag_dict in self.configure['tag_conf']:\n",
    "                    tag_name: str \n",
    "                    tag_words: List[str]\n",
    "                    tag_name, tag_words = list(tag_dict.items())[0]\n",
    "                    for word in tag_words:\n",
    "                        if word in self.title:\n",
    "                            self.tag = tag_name\n",
    "                            break\n",
    "                    if self.tag:\n",
    "                        break\n",
    "                else:\n",
    "                    # if tag cannot be found, mark it as `untagged`\n",
    "                    self.tag = 'untagged'\n",
    "                self.logger.debug(f'Page {self.page} Entry {self.index} ' \\\n",
    "                                  f'{self.date} tagged \"{self.tag}\"')\n",
    "        except Exception as e:\n",
    "            # exception here will not terminate the program\n",
    "            logger.exception(e)\n",
    "    \n",
    "    @classmethod\n",
    "    def get_tag_configure(cls, conf_file: Optional[str]) -> None:\n",
    "        if conf_file:\n",
    "            with open(conf_file, 'r', encoding='utf-8') as conf:\n",
    "                try:\n",
    "                    cls.configure = json.loads(conf.read())\n",
    "                    cls.logger.info(f'{conf_file} read for tag configure')\n",
    "                except Exception as e:\n",
    "                    cls.logger.exception(e)\n",
    "                    #cls.configure = dict()\n",
    "        else:\n",
    "            cls.logger.warning(f'no tag configure file provided, no tag process')\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = f'Page {self.page} Entry {self.index}\\n' \\\n",
    "            f'Tag: {self.tag}\\n' \\\n",
    "            f'Date: {self.date} {self.time}\\n' \\\n",
    "            f'{self.title}\\n' \\\n",
    "            f'{self.content}\\n'\n",
    "        return s\n",
    "    \n",
    "    __repr__ = __str__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Page` acts like a task. Each `Page` represents one page and contains the following:\n",
    "* use a page index to construct\n",
    "* url for request the page\n",
    "* method to request the page use `async_get()`\n",
    "* attribute to store both parsed and unparsed returns\n",
    "* attribute to contain `Entry` objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Page(SetLogger):\n",
    "    \n",
    "    page: int\n",
    "    url: str\n",
    "    unparsed_text: str\n",
    "    #parsed_test: str\n",
    "    entries: List[Entry]\n",
    "    \n",
    "    def __init__(self, page: int):\n",
    "        self.page = page\n",
    "        self.url = BASE_URL.format(page)\n",
    "        self.entries: List[Entry] = []\n",
    "    \n",
    "    async def request(self, \n",
    "                      session: aiohttp.ClientSession, \n",
    "                      logger: Optional[logging.Logger] = None) -> None:\n",
    "        if not logger:\n",
    "            logger = self.logger\n",
    "        self.unparsed_text = await async_get(\n",
    "            self.url, f'Page({self.page})', HEADERS, session, logger, text=True)\n",
    "    \n",
    "    def extract(self) -> None:\n",
    "        \"\"\"fill self.entries with extracted Entry objects\"\"\"\n",
    "        try:\n",
    "            html = BeautifulSoup(self.unparsed_text, 'lxml')\n",
    "            articles = html.find('section', class_='module-content') \\\n",
    "                           .find_all('article', class_='story')\n",
    "            for i, atcl in enumerate(articles):\n",
    "                try:\n",
    "                    title = atcl.find('h3', class_='story-title').text\n",
    "                    content = atcl.find('p').text\n",
    "                    time_ = atcl.find('span', class_='timestamp').text\n",
    "                    e = self._create_Entry(i, time_, title, content)\n",
    "                    e.get_tag()\n",
    "                    self.entries.append(e)\n",
    "                except Exception as e:\n",
    "                    # prevent terminating\n",
    "                    self.logger.exception(e)\n",
    "        except Exception as e:\n",
    "            # page scale prevent terminating other tasks\n",
    "            self.logger.exception(e)\n",
    "            \n",
    "    \n",
    "    def _create_Entry(self, index: int, \n",
    "                      date_or_time: str, title: str, content: str) -> Entry:\n",
    "        title = _replace(title)\n",
    "        content = _replace(content)\n",
    "        if ':' in date_or_time:\n",
    "            time_ = date_or_time\n",
    "        else:\n",
    "            time_ = ''\n",
    "            date = date_or_time\n",
    "        # add today as date if time is present\n",
    "        if time_:\n",
    "            td = datetime.today()\n",
    "            date = f'{td.year}年 {td.month}月 {td.day}日'\n",
    "        return Entry(self.page, index, time_, date, title, content)\n",
    "    \n",
    "    def __str__(self):\n",
    "        s = ''\n",
    "        for entry in self.entries:\n",
    "            s += str(entry) + '\\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test][20/04/23 14:52:56 554.825][INFO]-pid:18068 th_id:15612- Page(1) [200 OK 0.27139s] \n"
     ]
    }
   ],
   "source": [
    "# test for Page object\n",
    "_p1 = Page(1)\n",
    "_p1.logger = test_logger\n",
    "async with aiohttp.ClientSession() as s:\n",
    "    await _p1.request(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_p1.unparsed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Document`   \n",
    "* store and sort `Page` objects\n",
    " * `Page.index` will be used\n",
    "* output to file\n",
    " * one file for all content, ordered by `Page.index`\n",
    " * Each file for a tag, if tag exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document(SetLogger):\n",
    "    \n",
    "    pages: List[Page]\n",
    "    path: str\n",
    "    _timestamp: str\n",
    "    \n",
    "    fmt = '%Y-%m-%d_%H-%M-%S'\n",
    "    \n",
    "    def __init__(self, path: str):\n",
    "        self.pages: List[Page] = []\n",
    "        self.path = path\n",
    "        self._timestamp = ''\n",
    "    \n",
    "    def add(self, page: Page) -> None:\n",
    "        self.pages.append(page)\n",
    "    \n",
    "    def output(self, tag: bool = True) -> None:\n",
    "        # sort by Page.page\n",
    "        self.pages.sort(key=lambda p: p.page)\n",
    "        self._timestamp = datetime.now().strftime(self.fmt)\n",
    "        \n",
    "        self._output_total()\n",
    "        if tag:\n",
    "            self._output_tag()\n",
    "    \n",
    "    def _output_total(self) -> None:\n",
    "        # use timestamp as file name\n",
    "        f = os.path.join(self.path, self._timestamp+'.txt')\n",
    "        with open(f, 'w', encoding='utf-8') as output:\n",
    "            for p in self.pages:\n",
    "                output.write(str(p))\n",
    "        self.logger.info(f'total content output to {f}')\n",
    "    \n",
    "    def _output_tag(self) -> None:\n",
    "        tag_entry_dict: Dict[str, str] = dict()\n",
    "        # iterate through all Entry in all Page\n",
    "        for p in self.pages:\n",
    "            for e in p.entries:\n",
    "                if e.tag in tag_entry_dict.keys():\n",
    "                    tag_entry_dict[e.tag] += str(e) + '\\n'\n",
    "                else:\n",
    "                    tag_entry_dict[e.tag] = str(e) + '\\n'\n",
    "        # one file for each tag\n",
    "        for tag, entries_str in tag_entry_dict.items():\n",
    "            f = os.path.join(self.path, f'{self._timestamp}_{tag}.txt')\n",
    "            with open(f, 'a+', encoding='utf-8') as tag_output:\n",
    "                tag_output.write(entries_str)\n",
    "            self.logger.info(f'Tag {tag} output to {f}')\n",
    "        self.logger.info(f'All tags output finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### worker functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dispatch task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every Page works as a task, created by `dispatch()` and distributed to async queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispatch(page_count: int, \n",
    "             task_q: asyncio.Queue, \n",
    "             logger: logging.Logger) -> None:\n",
    "    for i in range(page_count):\n",
    "        p = Page(i)\n",
    "        task_q.put_nowait(p)\n",
    "    logger.info(f'{page_count} Page created')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`downloader` function:\n",
    "* coroutine\n",
    "* get a Page from async queue\n",
    " * implement `asyncio.wait_for()`, prevent deadlock\n",
    "* implement `q.task_done()` in `try: ... except: ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def downloader(index: int, \n",
    "                     task_q: asyncio.Queue,\n",
    "                     output_q: asyncio.Queue,\n",
    "                     session: aiohttp.ClientSession,\n",
    "                     delay: float) -> None:\n",
    "    t1 = time.time()\n",
    "    logger = get_logger(f'downloader_{index}')\n",
    "    logger.info(f'Worker{index} starts')\n",
    "    while task_q.qsize() > 0:\n",
    "        try:\n",
    "            page: Page = await asyncio.wait_for(task_q.get(), timeout=0.5)\n",
    "            await page.request(session, logger)\n",
    "            output_q.put_nowait(page)\n",
    "            task_q.task_done()\n",
    "            await asyncio.sleep(delay)\n",
    "        except asyncio.TimeoutError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            task_q.task_done()\n",
    "            logger.exception(e)\n",
    "    logger.info(f'Worker{index} finished, {time.time()-t1:.5f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`processor` function:\n",
    "* get `Page` object from output queue\n",
    "* `Page.extract()`\n",
    "* store extracted pages to a `Document` object\n",
    " * as a single thread async program, no race condition exits\n",
    "* should use `await asyncio.sleep()` to cede control back to the eventloop\n",
    "* *determine the exit condition before doing anything else*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def processor(index: int, \n",
    "                    output_q: asyncio.Queue, \n",
    "                    downloader_list: List[asyncio.Task],\n",
    "                    document: Document) -> None:\n",
    "    t1 = time.time()\n",
    "    logger = get_logger(f'processor_{index}')\n",
    "    logger.info(f'Processor{index} starts')\n",
    "    while True:\n",
    "        try:\n",
    "            # determine the exit condition before doing anything else\n",
    "            if all([task.done() for task in downloader_list]) and output_q.qsize() == 0:\n",
    "                break\n",
    "            page: Page = await asyncio.wait_for(output_q.get(), 0.5)\n",
    "            page.extract()\n",
    "            document.add(page)\n",
    "            # make sure to cede control back to the eventloop\n",
    "            await asyncio.sleep(0.1)\n",
    "        except asyncio.TimeoutError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            logger.exception(e)\n",
    "    logger.info(f'Processor{index} finished, {time.time()-t1:.5f}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`main()` coroutine:\n",
    "* check path\n",
    "* set main logger with base name first\n",
    " * then set logger for Entry, Page and Document\n",
    " * must set logger before having any operations on classes or their instances\n",
    "* reset Entry tag configure\n",
    "* create async queue objects\n",
    "* dispatch `Page`s as tasks\n",
    "* create downloader and processor `Task` objects\n",
    "* `aiohttp.ClientSession`\n",
    " * `try: ... finally: ...`\n",
    "* await task queue\n",
    "* gather all tasks, \n",
    " * to ensure all task is done before main thread exits\n",
    "* generate `Document` and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main(page_count: int, \n",
    "               delay: float, \n",
    "               downloader_count: int, \n",
    "               output_path: Optional[str],\n",
    "               tag: bool, \n",
    "               conf_file: Optional[str], \n",
    "               level: int) -> None:\n",
    "    logger = get_main_logger(base_name=LOGGER_NAME, level=level)\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # check output path\n",
    "    if not output_path:\n",
    "        output_path = OUTPUT_PATH\n",
    "    check_path(output_path, logger)\n",
    "    \n",
    "    # set class logger \n",
    "    Entry.set_logger('Entry')\n",
    "    Page.set_logger('Page')\n",
    "    Document.set_logger('Document')\n",
    "    \n",
    "    # reset Entry tag configure\n",
    "    Entry.get_tag_configure(conf_file)\n",
    "    \n",
    "    # generate queues\n",
    "    task_q: asyncio.Queue = asyncio.Queue()\n",
    "    output_q: asyncio.Queue = asyncio.Queue()\n",
    "    \n",
    "    # generate Pages\n",
    "    dispatch(page_count, task_q, logger)\n",
    "    \n",
    "    # await client session\n",
    "    s = aiohttp.ClientSession()\n",
    "    \n",
    "    # create downloader task\n",
    "    downloader_list: asyncio.Task = []\n",
    "    try:\n",
    "        for i in range(downloader_count):\n",
    "            downloader_list.append(\n",
    "                asyncio.create_task(\n",
    "                    downloader(i, task_q, output_q, s, delay)\n",
    "                )\n",
    "            )\n",
    "        # create processor task and global Document object\n",
    "        doc = Document(output_path)\n",
    "        processor_task = asyncio.create_task(\n",
    "            processor(1, output_q, downloader_list, doc)\n",
    "        )\n",
    "        # wait for task queue to be emptied, all created tasks will be \n",
    "        # scheduled and awaited implicitly\n",
    "        await task_q.join()\n",
    "        \n",
    "        # await until all tasks are done\n",
    "        task_list = downloader_list + [processor_task]\n",
    "        await asyncio.gather(*task_list)\n",
    "        \n",
    "        # output extracted pages and entries to file\n",
    "        doc.output()\n",
    "        \n",
    "        \n",
    "    finally:\n",
    "        await s.close()\n",
    "        logger.info('ClientSession closed.')\n",
    "        logger.info(f'all done, total time {time.time()-t1:.5f}s')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[cnR_async][20/04/23 14:55:34 943.424][INFO]-pid:18068 th_id:15612- path cnReuters_output already exists\n",
      "[cnR_async.Entry][20/04/23 14:55:34 945.420][INFO]-pid:18068 th_id:15612- tag_configure.json read for tag configure\n",
      "[cnR_async][20/04/23 14:55:34 946.416][INFO]-pid:18068 th_id:15612- 30 Page created\n",
      "[cnR_async.downloader_0][20/04/23 14:55:34 947.414][INFO]-pid:18068 th_id:15612- Worker0 starts\n",
      "[cnR_async.downloader_1][20/04/23 14:55:34 947.414][INFO]-pid:18068 th_id:15612- Worker1 starts\n",
      "[cnR_async.downloader_2][20/04/23 14:55:34 948.411][INFO]-pid:18068 th_id:15612- Worker2 starts\n",
      "[cnR_async.processor_1][20/04/23 14:55:34 948.411][INFO]-pid:18068 th_id:15612- Processor1 starts\n",
      "[cnR_async.downloader_1][20/04/23 14:55:35 003.865][INFO]-pid:18068 th_id:15612- Page(1) [200 OK 0.05446s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:35 393.180][INFO]-pid:18068 th_id:15612- Page(2) [200 OK 0.44275s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:35 393.180][INFO]-pid:18068 th_id:15612- Page(0) [200 OK 0.44377s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:37 312.702][INFO]-pid:18068 th_id:15612- Page(3) [200 OK 0.30115s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:37 683.848][INFO]-pid:18068 th_id:15612- Page(4) [200 OK 0.29376s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:37 719.706][INFO]-pid:18068 th_id:15612- Page(5) [200 OK 0.31266s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:39 538.973][INFO]-pid:18068 th_id:15612- Page(6) [200 OK 0.22110s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:39 932.131][INFO]-pid:18068 th_id:15612- Page(7) [200 OK 0.23429s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:40 017.098][INFO]-pid:18068 th_id:15612- Page(8) [200 OK 0.28801s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:41 840.292][INFO]-pid:18068 th_id:15612- Page(9) [200 OK 0.29704s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:42 180.878][INFO]-pid:18068 th_id:15612- Page(10) [200 OK 0.24668s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:42 330.506][INFO]-pid:18068 th_id:15612- Page(11) [200 OK 0.30233s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:44 151.437][INFO]-pid:18068 th_id:15612- Page(12) [200 OK 0.30386s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:44 428.017][INFO]-pid:18068 th_id:15612- Page(13) [200 OK 0.23742s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:44 907.997][INFO]-pid:18068 th_id:15612- Page(14) [200 OK 0.56861s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:46 465.271][INFO]-pid:18068 th_id:15612- Page(15) [200 OK 0.30012s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:46 664.281][INFO]-pid:18068 th_id:15612- Page(16) [200 OK 0.23328s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:47 351.622][INFO]-pid:18068 th_id:15612- Page(17) [200 OK 0.43011s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:48 711.527][INFO]-pid:18068 th_id:15612- Page(18) [200 OK 0.23432s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:48 960.533][INFO]-pid:18068 th_id:15612- Page(19) [200 OK 0.29582s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:49 635.003][INFO]-pid:18068 th_id:15612- Page(20) [200 OK 0.28711s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:51 010.963][INFO]-pid:18068 th_id:15612- Page(21) [200 OK 0.29675s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:51 292.186][INFO]-pid:18068 th_id:15612- Page(22) [200 OK 0.32803s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:51 934.077][INFO]-pid:18068 th_id:15612- Page(23) [200 OK 0.29685s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:53 326.565][INFO]-pid:18068 th_id:15612- Page(24) [200 OK 0.29677s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:53 543.215][INFO]-pid:18068 th_id:15612- Page(25) [200 OK 0.21665s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:54 182.185][INFO]-pid:18068 th_id:15612- Page(26) [200 OK 0.23405s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:55 634.239][INFO]-pid:18068 th_id:15612- Page(27) [200 OK 0.29907s] \n",
      "[cnR_async.downloader_2][20/04/23 14:55:55 800.915][INFO]-pid:18068 th_id:15612- Page(28) [200 OK 0.23058s] \n",
      "[cnR_async.downloader_0][20/04/23 14:55:56 406.115][INFO]-pid:18068 th_id:15612- Page(29) [200 OK 0.22827s] \n",
      "[cnR_async.downloader_1][20/04/23 14:55:57 643.800][INFO]-pid:18068 th_id:15612- Worker1 finished, 22.69639s\n",
      "[cnR_async.downloader_2][20/04/23 14:55:57 811.085][INFO]-pid:18068 th_id:15612- Worker2 finished, 22.86267s\n",
      "[cnR_async.downloader_0][20/04/23 14:55:58 426.129][INFO]-pid:18068 th_id:15612- Worker0 finished, 23.47971s\n",
      "[cnR_async.processor_1][20/04/23 14:55:58 563.839][INFO]-pid:18068 th_id:15612- Processor1 finished, 23.61543s\n",
      "[cnR_async.Document][20/04/23 14:55:58 568.050][INFO]-pid:18068 th_id:15612- total content output to cnReuters_output\\2020-04-23_14-55-58.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 能源 output to cnReuters_output\\2020-04-23_14-55-58_能源.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 亚太 output to cnReuters_output\\2020-04-23_14-55-58_亚太.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag untagged output to cnReuters_output\\2020-04-23_14-55-58_untagged.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 欧美 output to cnReuters_output\\2020-04-23_14-55-58_欧美.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 地缘政治 output to cnReuters_output\\2020-04-23_14-55-58_地缘政治.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 外汇 output to cnReuters_output\\2020-04-23_14-55-58_外汇.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 农产品 output to cnReuters_output\\2020-04-23_14-55-58_农产品.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 利率 output to cnReuters_output\\2020-04-23_14-55-58_利率.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 569.151][INFO]-pid:18068 th_id:15612- Tag 贵金属 output to cnReuters_output\\2020-04-23_14-55-58_贵金属.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 584.773][INFO]-pid:18068 th_id:15612- Tag 股指 output to cnReuters_output\\2020-04-23_14-55-58_股指.txt\n",
      "[cnR_async.Document][20/04/23 14:55:58 584.773][INFO]-pid:18068 th_id:15612- All tags output finished\n",
      "[cnR_async][20/04/23 14:55:58 584.773][INFO]-pid:18068 th_id:15612- ClientSession closed.\n",
      "[cnR_async][20/04/23 14:55:58 584.773][INFO]-pid:18068 th_id:15612- all done, total time 23.64135s\n"
     ]
    }
   ],
   "source": [
    "await main(30, 2, 3, None, True, 'tag_configure.json', logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
